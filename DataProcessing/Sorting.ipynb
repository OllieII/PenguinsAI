{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_seconds_from_launch(game_state_str):\n",
    "    try:\n",
    "        return json.loads(game_state_str).get('seconds_from_launch', 0)\n",
    "    except json.JSONDecodeError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\oguo2\\GitHub\\PenguinsAI\\RawData\\PENGUINS_20240101_to_20240131_df72162_events.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_then_launch(target_csv, output_name, output_status):\n",
    "    target_csv['seconds_from_launch'] = target_csv['game_state'].apply(extract_seconds_from_launch)\n",
    "    df_sorted = target_csv.sort_values(by=['session_id', 'index'])\n",
    "    if output_status:\n",
    "        df_sorted.to_csv(output_name, sep='\\t', index=False)\n",
    "    else:\n",
    "        return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = session_then_launch(df,\"Sorted_Jan\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['session_id', 'app_id', 'timestamp', 'event_name', 'event_data',\n",
       "       'event_source', 'app_version', 'app_branch', 'log_version', 'offset',\n",
       "       'user_id', 'user_data', 'game_state', 'index', 'seconds_from_launch'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_event_starts(group, event_name):\n",
    "    mismatches = []\n",
    "    # Get indices of the specified event\n",
    "    event_indices = group[group['event_name'] == event_name].index\n",
    "    # Check each event occurrence\n",
    "    for idx in event_indices:\n",
    "        # If it's not the first event or if the previous 'session_id' is the same, it's a mismatch\n",
    "        if idx != group.index[0] and group.at[idx, 'session_id'] == group.at[idx - 1, 'session_id']:\n",
    "            mismatches.append(group.at[idx, 'session_id'])\n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mismatches(df, event_name):\n",
    "    # Group by 'session_id' and apply the checking function for the specified event name\n",
    "    mismatched_sessions = df.groupby('session_id').apply(lambda g: check_event_starts(g, event_name))\n",
    "\n",
    "    # Flatten the list of mismatched sessions\n",
    "    mismatched_sessions = [item for sublist in mismatched_sessions for item in sublist]\n",
    "\n",
    "    # Print out the sessions with mismatches\n",
    "    if mismatched_sessions:\n",
    "        print(f\"The total of '{len(mismatched_sessions)}'following sessions have '{event_name}' events that do not match a change in 'session_id':\")\n",
    "        for session in mismatched_sessions:\n",
    "            print(session)\n",
    "    else:\n",
    "        print(f\"All '{event_name}' events match a change in 'session_id'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 'device_identifier' events match a change in 'session_id'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oguo2\\AppData\\Local\\Temp\\ipykernel_33336\\3782325686.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mismatched_sessions = df.groupby('session_id').apply(lambda g: check_event_starts(g, event_name))\n"
     ]
    }
   ],
   "source": [
    "find_mismatches(df_sorted, 'device_identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['index']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['event_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Adjusted function to skip 'egg_hatched' or 'nest_complete' events in each session\n",
    "def convert_to_lstm_input_format_adjusted(df):\n",
    "    # Initialize the one-hot encoder for event names, excluding the ones to be removed\n",
    "    valid_events = df[~df['event_name'].isin(['egg_hatched', 'nest_complete'])]['event_name'].unique()\n",
    "    event_encoder = OneHotEncoder()\n",
    "    event_encoder.fit(valid_events.reshape(-1, 1))\n",
    "\n",
    "    sessions = df['session_id'].unique()\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    for session in sessions:\n",
    "        session_data = df[df['session_id'] == session].sort_values('timestamp')\n",
    "        \n",
    "        sequence = []\n",
    "\n",
    "        for _, row in session_data.iterrows():\n",
    "            # Skip the 'egg_hatched' or 'nest_complete' events\n",
    "            if row['event_name'] in ['egg_hatched', 'nest_complete']:\n",
    "                continue\n",
    "\n",
    "            game_state = json.loads(row['game_state'])\n",
    "            \n",
    "            features = [\n",
    "                game_state.get('posX', 0),\n",
    "                game_state.get('posY', 0),\n",
    "                game_state.get('posZ', 0),\n",
    "                game_state.get('rotW', 1),\n",
    "                game_state.get('rotX', 0),\n",
    "                game_state.get('rotY', 0),\n",
    "                game_state.get('rotZ', 0),\n",
    "                game_state.get('seconds_from_launch', 0)\n",
    "            ]\n",
    "            \n",
    "            if row['event_name'] in valid_events:  # Check if event is in the list of valid events\n",
    "                event_encoded = event_encoder.transform([[row['event_name']]]).toarray()\n",
    "                features.extend(event_encoded.flatten().tolist())\n",
    "                sequence.append(features)\n",
    "        \n",
    "        if not sequence:  # Skip sessions that end up with no data after filtering\n",
    "            continue\n",
    "\n",
    "        # Normalize/Standardize the sequence features\n",
    "        scaler = StandardScaler()\n",
    "        sequence = np.array(sequence)\n",
    "        sequence[:, :8] = scaler.fit_transform(sequence[:, :8])\n",
    "\n",
    "        label = 1  # Assuming default label (adjust according to your logic)\n",
    "        \n",
    "        all_sequences.append(sequence)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    return all_sequences, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences, all_labels = convert_to_lstm_input_format(df=df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673036093418259"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels.count(1)/len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sequences and labels are the output from convert_to_lstm_input_format(df)\n",
    "# Convert sequences to tensors and pad them\n",
    "sequences_padded = pad_sequence([torch.tensor(s) for s in all_sequences], batch_first=True)\n",
    "labels_tensor = torch.tensor(all_labels)\n",
    "\n",
    "# Create a Dataset and DataLoader for batching\n",
    "dataset = TensorDataset(sequences_padded, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([471, 14406, 38])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the batch\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input)\n",
    "        out = self.dropout(ht[-1])\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the size of the inputs\n",
    "input_size = sequences_padded.size(2) # Number of features\n",
    "hidden_size = 64 # Can be tuned\n",
    "num_layers = 1 # Can be tuned\n",
    "num_classes = 1 # Binary classification\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy\n",
    "def binary_accuracy(preds, y):\n",
    "    # Round predictions to the closest integer (0 or 1)\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def calculate_lengths(sequences_batch):\n",
    "    # Calculate lengths based on the assumption that padding value is zero\n",
    "    lengths = (sequences_batch != 0).sum(dim=2)  # Sum over the feature dimension to get non-zero feature vectors\n",
    "    lengths = lengths.max(dim=1).values  # Find the maximum length in the feature vectors\n",
    "    # Sort the lengths in descending order and get sorting indices\n",
    "    lengths, sorted_idx = lengths.sort(descending=True)\n",
    "    return lengths, sorted_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/15], Loss: 0.6365, Accuracy: 0.72\n",
      "Epoch [2/10], Step [10/15], Loss: 0.6065, Accuracy: 0.62\n",
      "Epoch [3/10], Step [10/15], Loss: 0.5355, Accuracy: 0.72\n",
      "Epoch [4/10], Step [10/15], Loss: 0.3890, Accuracy: 0.88\n",
      "Epoch [5/10], Step [10/15], Loss: 0.5091, Accuracy: 0.72\n",
      "Epoch [6/10], Step [10/15], Loss: 0.5299, Accuracy: 0.69\n",
      "Epoch [7/10], Step [10/15], Loss: 0.3798, Accuracy: 0.81\n",
      "Epoch [8/10], Step [10/15], Loss: 0.4583, Accuracy: 0.81\n",
      "Epoch [9/10], Step [10/15], Loss: 0.2546, Accuracy: 0.91\n",
      "Epoch [10/10], Step [10/15], Loss: 0.3480, Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10 # Can be tuned\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sequences_batch, labels_batch) in enumerate(dataloader):\n",
    "        sequences_batch, labels_batch = sequences_batch, labels_batch\n",
    "        sequences_batch = sequences_batch.float()\n",
    "        labels_batch = labels_batch.float()\n",
    "        lengths, sorted_idx = calculate_lengths(sequences_batch)\n",
    "    \n",
    "        # You need to sort sequences_batch and labels_batch based on sorted_idx\n",
    "        sequences_batch = sequences_batch[sorted_idx]\n",
    "        labels_batch = labels_batch[sorted_idx]\n",
    "        # Forward pass\n",
    "        outputs = model(sequences_batch, lengths)\n",
    "    \n",
    "        loss = criterion(outputs.view(-1), labels_batch.float())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            acc = binary_accuracy(outputs.view(-1), labels_batch.float())\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "# You would also want to include validation and possibly early stopping in the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
